{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image_Classification.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"F4KRFW5AXTcg","colab_type":"text"},"cell_type":"markdown","source":["#CNNs for Image Classification"]},{"metadata":{"id":"wNOMBHz4XbSo","colab_type":"text"},"cell_type":"markdown","source":["##Digit Recognition\n","Overview of the problem:\n","* Aim: classify images of hand-written digits.\n","* Dataset: MNIST, 60,000 many 28x28 grayscale images.\n","* Model: a simple CNN.\n","* Criterion: Cross entropy loss.\n","* Optimizer: SGD or Adam.\n","\n","Strategy:\n","We will build progressively complex models starting from a very simple CNN, and utilizing more complex methods along the way. Here is a brief description of the models we are going to build:\n","1. Simple CNN.\n","1. Adding more layers.\n","1. Adding Dropout.\n","1. Adding Batch Normalization (BN).\n"]},{"metadata":{"id":"oIbJhVeppgw1","colab_type":"text"},"cell_type":"markdown","source":["#Setup Preparation"]},{"metadata":{"id":"42jaVSCUXSss","colab_type":"code","colab":{}},"cell_type":"code","source":["# import torch and torchvision\n","!pip install -q torch==1.0.0 torchvision"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gFANaUW8ZAOV","colab_type":"code","outputId":"4f5c932c-e347-468d-822c-a1dce0ebf8e9","executionInfo":{"status":"ok","timestamp":1551782750671,"user_tz":-360,"elapsed":2001,"user":{"displayName":"Birzhan Moldagaliyev","photoUrl":"https://lh6.googleusercontent.com/-bBMNk_lfHxc/AAAAAAAAAAI/AAAAAAAAALw/Z9rp6VxhrVw/s64/photo.jpg","userId":"07030169108509602369"}},"colab":{"base_uri":"https://localhost:8080/","height":1247}},"cell_type":"code","source":["# set up common infrastructure, such as data, training parameters and so on. \n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","# set parameters for loading data\n","torch.manual_seed(7)\n","batch_size = 8\n","\n","# define transform on a dataset\n","# the values for mean and standard deviation for MNIST dataset is from 'https://nextjournal.com/gkoehler/pytorch-mnist'.\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3081,))])\n","# load MNIST data available in torchvision\n","train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","# prepare batches for both trainset and testset\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","# let us look at some of the training images\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","#roll batches\n","batches = enumerate(train_loader)\n","batch_id, (images, labels) = next(batches)\n","\n","print(images)\n","\n","fig = plt.figure()\n","for i in range(8):\n","  plt.subplot(3,4,i+1)\n","  plt.tight_layout()\n","  plt.imshow(images[i][0], cmap='gray', interpolation='none')\n","  plt.title(\"Ground truth: {}\".format(labels[i]))\n","  plt.xticks([])\n","  plt.yticks([])\n","  \n","  \n","  \n","  \n","\n","\n"],"execution_count":37,"outputs":[{"output_type":"stream","text":["tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        ...,\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n","\n","\n","        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          ...,\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n","          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAh8AAAEACAYAAAATNRQCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 576x396 with 8 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"_O_bSotSpZ4q","colab_type":"text"},"cell_type":"markdown","source":["#Constructing models\n","\n","\n"]},{"metadata":{"id":"O3FCOt-ChamZ","colab_type":"code","colab":{}},"cell_type":"code","source":["# shallow CNN\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN_V1(nn.Module):\n","  def __init__(self):\n","    super(CNN_V1,self).__init__()\n","    self.conv = nn.Conv2d(1,3,3)\n","    self.fc = nn.Linear(3*26*26,10)\n","    \n","  def forward(self, x):\n","    x = F.relu(self.conv(x))\n","    x = x.view(-1, 3*26*26)\n","    x = self.fc(x)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dzp6RWXjRqAk","colab_type":"code","colab":{}},"cell_type":"code","source":["# adding more layers\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN_V2(nn.Module):\n","  def __init__(self):\n","    super(CNN_V2,self).__init__()\n","    self.conv1 = nn.Conv2d(1,3,3)\n","    self.conv2 = nn.Conv2d(3,6,3)\n","    self.fc1 = nn.Linear(6*24*24,100)\n","    self.fc2 = nn.Linear(100,10)\n","    \n","    \n","  def forward(self, x):\n","    x = F.relu(self.conv1(x))\n","    x = F.relu(self.conv2(x))\n","    x = x.view(-1, 6*24*24)\n","    x = F.relu(self.fc1(x))\n","    x = self.fc2(x)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5qwi1x5ETDhd","colab_type":"code","colab":{}},"cell_type":"code","source":["# adding more layers and dropout\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN_V3(nn.Module):\n","  def __init__(self):\n","    super(CNN_V3,self).__init__()\n","    self.conv1 = nn.Conv2d(1,3,3)\n","    self.conv2 = nn.Conv2d(3,6,3)\n","    self.conv3 = nn.Conv2d(6,15,3)\n","    self.conv3_drop = nn.Dropout2d()\n","    self.fc1 = nn.Linear(15*22*22,100)\n","    self.fc1_drop = nn.Dropout()\n","    self.fc2 = nn.Linear(100,10)\n","    \n","    \n","  def forward(self, x):\n","    x = F.relu(self.conv1(x))\n","    x = F.relu(self.conv2(x))\n","    x = self.conv3_drop(F.relu(self.conv3(x)))\n","    x = x.view(-1, 15*22*22)\n","    x = F.relu(self.fc1(x))\n","    x = self.fc1_drop(x)\n","    x = self.fc2(x)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vPeXee3IWvgX","colab_type":"code","colab":{}},"cell_type":"code","source":["#adding more layers and batch normalization\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN_V4(nn.Module):\n","  def __init__(self):\n","    super(CNN_V4,self).__init__()\n","    self.conv1 = nn.Conv2d(1,3,3)\n","    self.conv2 = nn.Conv2d(3,6,3)\n","    self.conv3 = nn.Conv2d(6,15,3)\n","    self.conv3_bn = nn.BatchNorm2d(15)\n","    self.conv4 = nn.Conv2d(15,30,3)\n","    self.conv4_bn = nn.BatchNorm2d(30)\n","    self.conv4_drop = nn.Dropout2d()\n","    self.fc1 = nn.Linear(30*20*20,500)\n","    self.fc1_bn = nn.BatchNorm1d(500)\n","    self.fc1_drop = nn.Dropout()\n","    self.fc2 = nn.Linear(500,100)\n","    self.fc2_drop = nn.Dropout()\n","    self.fc3 = nn.Linear(100,10)\n","    \n","    \n","  def forward(self, x):\n","    x = F.relu(self.conv1(x))\n","    x = F.relu(self.conv2(x))\n","    x = F.relu(self.conv3_bn(self.conv3(x)))\n","    x = self.conv4_bn(self.conv4(x))\n","    x = self.conv4_drop(F.relu(x))\n","    x = x.view(-1, 30*20*20)\n","    x = self.fc1_bn(self.fc1(x))\n","    x = self.fc1_drop(F.relu(x))\n","    x = self.fc2_drop(F.relu(self.fc2(x)))\n","    x = self.fc3(x)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VOmNTkx0cWwx","colab_type":"text"},"cell_type":"markdown","source":["#Initiating and Training models\n"]},{"metadata":{"id":"YE4tGGHOoYb9","colab_type":"code","outputId":"131d1945-829e-4651-ca38-51a447989dd3","executionInfo":{"status":"error","timestamp":1551783089809,"user_tz":-360,"elapsed":1314,"user":{"displayName":"Birzhan Moldagaliyev","photoUrl":"https://lh6.googleusercontent.com/-bBMNk_lfHxc/AAAAAAAAAAI/AAAAAAAAALw/Z9rp6VxhrVw/s64/photo.jpg","userId":"07030169108509602369"}},"colab":{"base_uri":"https://localhost:8080/","height":863}},"cell_type":"code","source":["cnn_v1 = CNN_V1()\n","cnn_v2 = CNN_V2()\n","cnn_v3 = CNN_V3()\n","cnn_v4 = CNN_V4()\n","# let's do a single forward prop as a sanity check\n","x = torch.rand(1,1,28,28)\n","out_v1 = cnn_v1(x)\n","out_v2 = cnn_v2(x)\n","out_v3 = cnn_v3(x)\n","out_v4 = cnn_v4(x)\n","print(\"Output for V1: {}\".format(out_v1))\n","print(\"Output for V2: {}\".format(out_v2))\n","print(\"Output for V3: {}\".format(out_v3))\n","print(\"Output for V4: {}\".format(out_v4))"],"execution_count":43,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-7f8682261abe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mout_v2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mout_v3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_v3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mout_v4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_v4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output for V1: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_v1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output for V2: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_v2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-b9f635f1c674>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1617\u001b[0m             \u001b[0msize_prods\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected more than 1 value per channel when training, got input size {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m     return torch.batch_norm(\n","\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 500])"]}]},{"metadata":{"id":"yGs-avouorzi","colab_type":"code","colab":{}},"cell_type":"code","source":["# define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(cnn_v1.parameters(), lr=0.001)\n","\n","# define number of epochs, i.e. number of \n","epochs = 2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YCxqVGR4o18n","colab_type":"code","outputId":"d57c396c-7b5e-4aa9-b2bc-6caf38c45280","executionInfo":{"status":"ok","timestamp":1551776549839,"user_tz":-360,"elapsed":214231,"user":{"displayName":"Birzhan Moldagaliyev","photoUrl":"https://lh6.googleusercontent.com/-bBMNk_lfHxc/AAAAAAAAAAI/AAAAAAAAALw/Z9rp6VxhrVw/s64/photo.jpg","userId":"07030169108509602369"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"cell_type":"code","source":["# training\n","for epoch in range(epochs):\n","  running_loss = 0.0\n","  for i, data in enumerate(train_loader, 0):\n","    # get inputs\n","    inputs, labels = data\n","    \n","    # zero parameter gradients\n","    optimizer.zero_grad()\n","    \n","    # forward + backward + optimize\n","    outputs = cnn_v1(inputs)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    \n","    # printing statistics\n","    running_loss += loss.item()\n","    if i%1000 == 999:\n","      print('Epoch: {}, Batch: {} - Loss: {}'.format(epoch, i+1, running_loss/1000))\n","      running_loss = 0\n","print('Finished Training')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Epoch: 0, Batch: 1000 - Loss: 4.381936936378479\n","Epoch: 0, Batch: 2000 - Loss: 4.217115129470825\n","Epoch: 0, Batch: 3000 - Loss: 4.097155544281006\n","Epoch: 0, Batch: 4000 - Loss: 3.975983458995819\n","Epoch: 0, Batch: 5000 - Loss: 3.888203641176224\n","Epoch: 0, Batch: 6000 - Loss: 3.8627274570465087\n","Epoch: 1, Batch: 1000 - Loss: 3.7490809235572815\n","Epoch: 1, Batch: 2000 - Loss: 3.7242246358394624\n","Epoch: 1, Batch: 3000 - Loss: 3.6744369032382966\n","Epoch: 1, Batch: 4000 - Loss: 3.7015702848434446\n","Epoch: 1, Batch: 5000 - Loss: 3.6336449456214903\n","Epoch: 1, Batch: 6000 - Loss: 3.6162652995586395\n","Finished Training\n"],"name":"stdout"}]},{"metadata":{"id":"xpzIujX2tYHw","colab_type":"text"},"cell_type":"markdown","source":["## Comments on Training Process:\n","* Training error did not drop much: need to use more complex models.\n","* Training process was very long: use GPUs"]},{"metadata":{"id":"lS5eyGNNo_dv","colab_type":"code","outputId":"1e832341-682d-493c-bb6c-47bf73d8f295","executionInfo":{"status":"ok","timestamp":1551686378213,"user_tz":-360,"elapsed":12477,"user":{"displayName":"Birzhan Moldagaliyev","photoUrl":"https://lh6.googleusercontent.com/-bBMNk_lfHxc/AAAAAAAAAAI/AAAAAAAAALw/Z9rp6VxhrVw/s64/photo.jpg","userId":"07030169108509602369"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# testing trained cnn_v1\n","total = 0\n","correct = 0\n","with torch.no_grad():\n","  for data in test_loader:\n","    inputs, labels = data\n","    outputs = cnn_v1(inputs)\n","    _, predicted = torch.max(outputs.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","print('Accuracy of the network on 10,000 images is {}'.format(correct/total))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy of the network on 10,000 images is 0.951\n"],"name":"stdout"}]},{"metadata":{"id":"vUjW-bwwuusa","colab_type":"text"},"cell_type":"markdown","source":["##Comments on Testing Process\n","* Though cnn_v1 looked weak during training, it produced 95.1% accuracy on the training set."]},{"metadata":{"id":"GRWqIhOZvQUi","colab_type":"text"},"cell_type":"markdown","source":["#Adding more layers"]},{"metadata":{"id":"xFIdgJEGvc45","colab_type":"code","colab":{}},"cell_type":"code","source":["# let's add few more layers\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN_V2(nn.Module):\n","  def __init__(self):\n","    super(CNN_V2,self).__init__()\n","    self.conv1 = nn.Conv2d(1,6,3)\n","    self.conv2 = nn.Conv2d(6,10,3)\n","    self.conv3 = nn.Conv2d(10,15,3)\n","    self.fc1 = nn.Linear(15*22*22,500)\n","    self.fc2 = nn.Linear(500,100)\n","    self.fc3 = nn.Linear(100,10)\n","    \n","  def forward(self, x):\n","    x = F.relu(self.conv1(x))\n","    x = F.relu(self.conv2(x))\n","    x = F.relu(self.conv3(x))\n","    x = x.view(-1, 15*22*22)\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    x = self.fc3(x)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5LshlLwsv5nj","colab_type":"code","outputId":"e88e2c6f-89b6-474d-9556-22d948e4bf99","executionInfo":{"status":"ok","timestamp":1551688117821,"user_tz":-360,"elapsed":825,"user":{"displayName":"Birzhan Moldagaliyev","photoUrl":"https://lh6.googleusercontent.com/-bBMNk_lfHxc/AAAAAAAAAAI/AAAAAAAAALw/Z9rp6VxhrVw/s64/photo.jpg","userId":"07030169108509602369"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["cnn_v2 = CNN_V2()\n","# forward pass for a sanity check\n","x = torch.rand(1,1,28,28)\n","out = cnn_v2(x)\n","print(out)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[-0.0892, -0.0061,  0.0226, -0.0412, -0.0960,  0.0950, -0.0103, -0.0221,\n","         -0.0303, -0.0252]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"metadata":{"id":"1ClPUYjWxvAx","colab_type":"code","colab":{}},"cell_type":"code","source":["# define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(cnn_v1.parameters(), lr=0.001)\n","\n","# define number of epochs, i.e. number of \n","epochs = 2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JaGT5GoVwOnv","colab_type":"code","outputId":"d174dfc9-b3ca-4646-b40c-813286371a0c","executionInfo":{"status":"ok","timestamp":1551688258736,"user_tz":-360,"elapsed":133611,"user":{"displayName":"Birzhan Moldagaliyev","photoUrl":"https://lh6.googleusercontent.com/-bBMNk_lfHxc/AAAAAAAAAAI/AAAAAAAAALw/Z9rp6VxhrVw/s64/photo.jpg","userId":"07030169108509602369"}},"colab":{"base_uri":"https://localhost:8080/","height":561}},"cell_type":"code","source":["# let's use gpu's for training\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","cnn_v2_gpu = cnn_v2.to(device)\n","\n","# training with gpu's\n","for epoch in range(epochs):\n","  running_loss = 0.0\n","  for i, data in enumerate(train_loader, 0):\n","    # get inputs\n","    inputs, labels = data\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","    \n","    # zero parameter gradients\n","    optimizer.zero_grad()\n","    \n","    # forward + backward + optimize\n","    outputs = cnn_v2_gpu(inputs)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    \n","    # printing statistics\n","    running_loss += loss.item()\n","    if i%500 == 499:\n","      print('Epoch: {}, Batch: {} - Loss: {}'.format(epoch, i+1, running_loss/500))\n","      running_loss = 0\n","print('Finished Training')\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda:0\n","Epoch: 0, Batch: 500 - Loss: 2.304574866771698\n","Epoch: 0, Batch: 1000 - Loss: 2.3053252067565917\n","Epoch: 0, Batch: 1500 - Loss: 2.3048322396278382\n","Epoch: 0, Batch: 2000 - Loss: 2.305904438495636\n","Epoch: 0, Batch: 2500 - Loss: 2.305352098941803\n","Epoch: 0, Batch: 3000 - Loss: 2.3052590651512146\n","Epoch: 0, Batch: 3500 - Loss: 2.3047765750885008\n","Epoch: 0, Batch: 4000 - Loss: 2.306828869342804\n","Epoch: 0, Batch: 4500 - Loss: 2.305187074184418\n","Epoch: 0, Batch: 5000 - Loss: 2.3054191184043886\n","Epoch: 0, Batch: 5500 - Loss: 2.3053304901123046\n","Epoch: 0, Batch: 6000 - Loss: 2.305799864768982\n","Epoch: 0, Batch: 6500 - Loss: 2.3050826783180236\n","Epoch: 0, Batch: 7000 - Loss: 2.305017894268036\n","Epoch: 0, Batch: 7500 - Loss: 2.3035001573562623\n","Epoch: 1, Batch: 500 - Loss: 2.3045323376655578\n","Epoch: 1, Batch: 1000 - Loss: 2.3064477105140684\n","Epoch: 1, Batch: 1500 - Loss: 2.305724175453186\n","Epoch: 1, Batch: 2000 - Loss: 2.306945587158203\n","Epoch: 1, Batch: 2500 - Loss: 2.3047574834823608\n","Epoch: 1, Batch: 3000 - Loss: 2.3053735423088075\n","Epoch: 1, Batch: 3500 - Loss: 2.3038794679641725\n","Epoch: 1, Batch: 4000 - Loss: 2.304065353393555\n","Epoch: 1, Batch: 4500 - Loss: 2.30480561876297\n","Epoch: 1, Batch: 5000 - Loss: 2.306768807411194\n","Epoch: 1, Batch: 5500 - Loss: 2.3046183195114134\n","Epoch: 1, Batch: 6000 - Loss: 2.3058336482048034\n","Epoch: 1, Batch: 6500 - Loss: 2.3035207290649415\n","Epoch: 1, Batch: 7000 - Loss: 2.306023671627045\n","Epoch: 1, Batch: 7500 - Loss: 2.3048941326141357\n","Finished Training\n"],"name":"stdout"}]},{"metadata":{"id":"bo0t3kgsmygy","colab_type":"code","colab":{}},"cell_type":"code","source":["# let's build a simple CNN\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class simpleCNN(nn.Module):\n","  def __init__(self):\n","    super(simpleCNN,self).__init__()\n","    self.conv1 = nn.Conv2d(1,6,3)\n","    self.conv2 = nn.Conv2d(6,10,3)\n","    self.conv3 = nn.Conv2d(10,15,3)\n","    self.conv3_drop = nn.Dropout2d()\n","    self.fc1 = nn.Linear(15*22*22,500)\n","    self.fc2 = nn.Linear(500,100)\n","    self.fc3 = nn.Linear(100,10)\n","    self.fc_drop = nn.Dropout()\n","    \n","  def forward(self, x):\n","    x = F.relu(self.conv1(x))\n","    x = F.relu(self.conv2(x))\n","    x = self.conv3_drop(F.relu(self.conv3(x)))\n","    x = x.view(-1, 15*22*22)\n","    x = F.relu(self.fc1(x))\n","    x = self.fc_drop(F.relu(self.fc2(x)))\n","    x = self.fc3(x)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jczga--t5C0x","colab_type":"code","outputId":"2fb3d488-b838-47f4-88bc-bb6ce1915517","executionInfo":{"status":"ok","timestamp":1551539937811,"user_tz":-360,"elapsed":1066,"user":{"displayName":"Birzhan Moldagaliyev","photoUrl":"https://lh6.googleusercontent.com/-bBMNk_lfHxc/AAAAAAAAAAI/AAAAAAAAALw/Z9rp6VxhrVw/s64/photo.jpg","userId":"07030169108509602369"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["model = simpleCNN()\n","x = torch.rand(1,1,28,28)\n","out = model(x)\n","print(out)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[ 0.0092,  0.0147, -0.0975, -0.0603, -0.0372, -0.0172,  0.0542,  0.0905,\n","         -0.0865, -0.0768]], grad_fn=<AddmmBackward>)\n"],"name":"stdout"}]},{"metadata":{"id":"K6mrQAVH6Y6D","colab_type":"code","colab":{}},"cell_type":"code","source":["# define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","epochs = 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sead6T4q7PhC","colab_type":"code","outputId":"6d390dc4-f995-49a9-c7c4-a99aa96b6bcb","executionInfo":{"status":"error","timestamp":1551539577322,"user_tz":-360,"elapsed":105772,"user":{"displayName":"Birzhan Moldagaliyev","photoUrl":"https://lh6.googleusercontent.com/-bBMNk_lfHxc/AAAAAAAAAAI/AAAAAAAAALw/Z9rp6VxhrVw/s64/photo.jpg","userId":"07030169108509602369"}},"colab":{"base_uri":"https://localhost:8080/","height":592}},"cell_type":"code","source":["# training\n","for epoch in range(epochs):\n","  running_loss = 0.0\n","  for i, data in enumerate(train_loader, 0):\n","    # get inputs\n","    inputs, labels = data\n","    \n","    # zero parameter gradients\n","    optimizer.zero_grad()\n","    \n","    # forward + backward + optimize\n","    outputs = model(inputs)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    \n","    # printing statistics\n","    running_loss += loss.item()\n","    if i%200 == 199:\n","      print('Epoch: {}, Batch: {} - Loss: {}'.format(epoch, i, running_loss/2000))\n","      running_loss = 0\n","print('Finished Training')\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 0, Batch: 199 - Loss: 0.1606016823798418\n","Epoch: 0, Batch: 399 - Loss: 0.09473657794296741\n","Epoch: 0, Batch: 599 - Loss: 0.08131995962560176\n","Epoch: 0, Batch: 799 - Loss: 0.06809577090293169\n","Epoch: 0, Batch: 999 - Loss: 0.058189478434622285\n","Epoch: 0, Batch: 1199 - Loss: 0.052750581294298174\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-4485ef530fda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"4byFv5CL936z","colab_type":"code","outputId":"0918fd1b-1913-49ea-aaaa-8249312d0710","executionInfo":{"status":"ok","timestamp":1551539946066,"user_tz":-360,"elapsed":3588,"user":{"displayName":"Birzhan Moldagaliyev","photoUrl":"https://lh6.googleusercontent.com/-bBMNk_lfHxc/AAAAAAAAAAI/AAAAAAAAALw/Z9rp6VxhrVw/s64/photo.jpg","userId":"07030169108509602369"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","model_gpu = model.to(device)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"metadata":{"id":"VxRFoPsJ_NXf","colab_type":"code","outputId":"48a013c7-6113-4471-9aa0-6b2a3c2a1973","executionInfo":{"status":"ok","timestamp":1551540720842,"user_tz":-360,"elapsed":716300,"user":{"displayName":"Birzhan Moldagaliyev","photoUrl":"https://lh6.googleusercontent.com/-bBMNk_lfHxc/AAAAAAAAAAI/AAAAAAAAALw/Z9rp6VxhrVw/s64/photo.jpg","userId":"07030169108509602369"}},"colab":{"base_uri":"https://localhost:8080/","height":6324}},"cell_type":"code","source":["# training on gpu's\n","for epoch in range(epochs):\n","  running_loss = 0.0\n","  for i, data in enumerate(train_loader, 0):\n","    # get inputs\n","    inputs, labels = data\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","    \n","    # zero parameter gradients\n","    optimizer.zero_grad()\n","    \n","    # forward + backward + optimize\n","    outputs = model(inputs)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    \n","    # printing statistics\n","    running_loss += loss.item()\n","    if i%200 == 199:\n","      print('Epoch: {}, Batch: {} - Loss: {}'.format(epoch, i, running_loss/2000))\n","      running_loss = 0\n","print('Finished Training')\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 0, Batch: 199 - Loss: 0.22619755971431732\n","Epoch: 0, Batch: 399 - Loss: 0.1534698329716921\n","Epoch: 0, Batch: 599 - Loss: 0.10687925186753273\n","Epoch: 0, Batch: 799 - Loss: 0.08755979834496976\n","Epoch: 0, Batch: 999 - Loss: 0.07356348738074303\n","Epoch: 0, Batch: 1199 - Loss: 0.06440420345962047\n","Epoch: 0, Batch: 1399 - Loss: 0.05879504141956568\n","Epoch: 0, Batch: 1599 - Loss: 0.050387114867568014\n","Epoch: 0, Batch: 1799 - Loss: 0.05279903920739889\n","Epoch: 0, Batch: 1999 - Loss: 0.04583788900822401\n","Epoch: 0, Batch: 2199 - Loss: 0.042096381202340126\n","Epoch: 0, Batch: 2399 - Loss: 0.03987231086939573\n","Epoch: 0, Batch: 2599 - Loss: 0.03486914905905723\n","Epoch: 0, Batch: 2799 - Loss: 0.04064674615114927\n","Epoch: 0, Batch: 2999 - Loss: 0.03417990254610777\n","Epoch: 0, Batch: 3199 - Loss: 0.03945361467450857\n","Epoch: 0, Batch: 3399 - Loss: 0.03461999867111445\n","Epoch: 0, Batch: 3599 - Loss: 0.03325802195817232\n","Epoch: 0, Batch: 3799 - Loss: 0.031181019999086856\n","Epoch: 0, Batch: 3999 - Loss: 0.03332561198621988\n","Epoch: 0, Batch: 4199 - Loss: 0.030704018861055372\n","Epoch: 0, Batch: 4399 - Loss: 0.026228831470012665\n","Epoch: 0, Batch: 4599 - Loss: 0.02649871139973402\n","Epoch: 0, Batch: 4799 - Loss: 0.029584098920226096\n","Epoch: 0, Batch: 4999 - Loss: 0.02621182906627655\n","Epoch: 0, Batch: 5199 - Loss: 0.024656962163746358\n","Epoch: 0, Batch: 5399 - Loss: 0.023809052221477032\n","Epoch: 0, Batch: 5599 - Loss: 0.02857872672379017\n","Epoch: 0, Batch: 5799 - Loss: 0.024649195462465287\n","Epoch: 0, Batch: 5999 - Loss: 0.026110598556697368\n","Epoch: 0, Batch: 6199 - Loss: 0.024010093189775945\n","Epoch: 0, Batch: 6399 - Loss: 0.02329664821177721\n","Epoch: 0, Batch: 6599 - Loss: 0.02509867738187313\n","Epoch: 0, Batch: 6799 - Loss: 0.019925875894725324\n","Epoch: 0, Batch: 6999 - Loss: 0.02022994439303875\n","Epoch: 0, Batch: 7199 - Loss: 0.023049353048205375\n","Epoch: 0, Batch: 7399 - Loss: 0.02319363047927618\n","Epoch: 1, Batch: 199 - Loss: 0.021277702808380126\n","Epoch: 1, Batch: 399 - Loss: 0.020769250854849815\n","Epoch: 1, Batch: 599 - Loss: 0.01968497958034277\n","Epoch: 1, Batch: 799 - Loss: 0.016759460262954235\n","Epoch: 1, Batch: 999 - Loss: 0.023924431748688222\n","Epoch: 1, Batch: 1199 - Loss: 0.01694106897711754\n","Epoch: 1, Batch: 1399 - Loss: 0.02184754755347967\n","Epoch: 1, Batch: 1599 - Loss: 0.016409043475985527\n","Epoch: 1, Batch: 1799 - Loss: 0.018084756687283515\n","Epoch: 1, Batch: 1999 - Loss: 0.01974538393318653\n","Epoch: 1, Batch: 2199 - Loss: 0.017946472220122814\n","Epoch: 1, Batch: 2399 - Loss: 0.016961532175540923\n","Epoch: 1, Batch: 2599 - Loss: 0.01595813099294901\n","Epoch: 1, Batch: 2799 - Loss: 0.015540450297296048\n","Epoch: 1, Batch: 2999 - Loss: 0.017706084571778774\n","Epoch: 1, Batch: 3199 - Loss: 0.019379637859761715\n","Epoch: 1, Batch: 3399 - Loss: 0.0145101775303483\n","Epoch: 1, Batch: 3599 - Loss: 0.018196441978216172\n","Epoch: 1, Batch: 3799 - Loss: 0.015309988655149936\n","Epoch: 1, Batch: 3999 - Loss: 0.01752104386687279\n","Epoch: 1, Batch: 4199 - Loss: 0.01709891866147518\n","Epoch: 1, Batch: 4399 - Loss: 0.016514109969139098\n","Epoch: 1, Batch: 4599 - Loss: 0.014804504290223121\n","Epoch: 1, Batch: 4799 - Loss: 0.018309109747409822\n","Epoch: 1, Batch: 4999 - Loss: 0.016089154832065106\n","Epoch: 1, Batch: 5199 - Loss: 0.015156471908092498\n","Epoch: 1, Batch: 5399 - Loss: 0.011416955552995205\n","Epoch: 1, Batch: 5599 - Loss: 0.015318002052605152\n","Epoch: 1, Batch: 5799 - Loss: 0.014926976852118969\n","Epoch: 1, Batch: 5999 - Loss: 0.012474350452423096\n","Epoch: 1, Batch: 6199 - Loss: 0.013439217321574689\n","Epoch: 1, Batch: 6399 - Loss: 0.013341574795544148\n","Epoch: 1, Batch: 6599 - Loss: 0.014867382876574993\n","Epoch: 1, Batch: 6799 - Loss: 0.010483934491872788\n","Epoch: 1, Batch: 6999 - Loss: 0.015966755703091622\n","Epoch: 1, Batch: 7199 - Loss: 0.01672207295894623\n","Epoch: 1, Batch: 7399 - Loss: 0.013212357863783837\n","Epoch: 2, Batch: 199 - Loss: 0.0159577122554183\n","Epoch: 2, Batch: 399 - Loss: 0.011847839340567589\n","Epoch: 2, Batch: 599 - Loss: 0.012298047356307507\n","Epoch: 2, Batch: 799 - Loss: 0.011649795450270176\n","Epoch: 2, Batch: 999 - Loss: 0.013713205844163894\n","Epoch: 2, Batch: 1199 - Loss: 0.014116585947573186\n","Epoch: 2, Batch: 1399 - Loss: 0.01260214601457119\n","Epoch: 2, Batch: 1599 - Loss: 0.012970273695886134\n","Epoch: 2, Batch: 1799 - Loss: 0.016643459036946296\n","Epoch: 2, Batch: 1999 - Loss: 0.013277051426470279\n","Epoch: 2, Batch: 2199 - Loss: 0.013577404193580151\n","Epoch: 2, Batch: 2399 - Loss: 0.01131370797753334\n","Epoch: 2, Batch: 2599 - Loss: 0.011448387086391449\n","Epoch: 2, Batch: 2799 - Loss: 0.010477137364447118\n","Epoch: 2, Batch: 2999 - Loss: 0.008780451275408267\n","Epoch: 2, Batch: 3199 - Loss: 0.011698396116495133\n","Epoch: 2, Batch: 3399 - Loss: 0.010817137025296688\n","Epoch: 2, Batch: 3599 - Loss: 0.010892475105822086\n","Epoch: 2, Batch: 3799 - Loss: 0.013577698446810246\n","Epoch: 2, Batch: 3999 - Loss: 0.011115734875202178\n","Epoch: 2, Batch: 4199 - Loss: 0.009300494685769082\n","Epoch: 2, Batch: 4399 - Loss: 0.013551245361566544\n","Epoch: 2, Batch: 4599 - Loss: 0.01050776233524084\n","Epoch: 2, Batch: 4799 - Loss: 0.009779869675636292\n","Epoch: 2, Batch: 4999 - Loss: 0.011532236509025098\n","Epoch: 2, Batch: 5199 - Loss: 0.010150739200413227\n","Epoch: 2, Batch: 5399 - Loss: 0.011955020487308503\n","Epoch: 2, Batch: 5599 - Loss: 0.00956055610626936\n","Epoch: 2, Batch: 5799 - Loss: 0.009961861148476601\n","Epoch: 2, Batch: 5999 - Loss: 0.01148167087137699\n","Epoch: 2, Batch: 6199 - Loss: 0.01088267333805561\n","Epoch: 2, Batch: 6399 - Loss: 0.011588712811470031\n","Epoch: 2, Batch: 6599 - Loss: 0.012991539642214774\n","Epoch: 2, Batch: 6799 - Loss: 0.01049737636744976\n","Epoch: 2, Batch: 6999 - Loss: 0.008047661356627942\n","Epoch: 2, Batch: 7199 - Loss: 0.009862207271158696\n","Epoch: 2, Batch: 7399 - Loss: 0.011356502406299113\n","Epoch: 3, Batch: 199 - Loss: 0.009144665770232678\n","Epoch: 3, Batch: 399 - Loss: 0.006582341484725475\n","Epoch: 3, Batch: 599 - Loss: 0.013685844272375108\n","Epoch: 3, Batch: 799 - Loss: 0.009181402519345284\n","Epoch: 3, Batch: 999 - Loss: 0.00684605012089014\n","Epoch: 3, Batch: 1199 - Loss: 0.007609581284224987\n","Epoch: 3, Batch: 1399 - Loss: 0.011332440182566643\n","Epoch: 3, Batch: 1599 - Loss: 0.011423521272838116\n","Epoch: 3, Batch: 1799 - Loss: 0.0096111067160964\n","Epoch: 3, Batch: 1999 - Loss: 0.009288801610469818\n","Epoch: 3, Batch: 2199 - Loss: 0.0080439021140337\n","Epoch: 3, Batch: 2399 - Loss: 0.010321291990578175\n","Epoch: 3, Batch: 2599 - Loss: 0.0072203010022640225\n","Epoch: 3, Batch: 2799 - Loss: 0.008601623743772506\n","Epoch: 3, Batch: 2999 - Loss: 0.008944207705557346\n","Epoch: 3, Batch: 3199 - Loss: 0.00960152094066143\n","Epoch: 3, Batch: 3399 - Loss: 0.011354692727327347\n","Epoch: 3, Batch: 3599 - Loss: 0.010264057204127312\n","Epoch: 3, Batch: 3799 - Loss: 0.008088319070637225\n","Epoch: 3, Batch: 3999 - Loss: 0.0071863549128174786\n","Epoch: 3, Batch: 4199 - Loss: 0.007252302974462509\n","Epoch: 3, Batch: 4399 - Loss: 0.008420073688030242\n","Epoch: 3, Batch: 4599 - Loss: 0.009142475202679634\n","Epoch: 3, Batch: 4799 - Loss: 0.010567682914435864\n","Epoch: 3, Batch: 4999 - Loss: 0.00960877811163664\n","Epoch: 3, Batch: 5199 - Loss: 0.01223541598021984\n","Epoch: 3, Batch: 5399 - Loss: 0.0065047087892889975\n","Epoch: 3, Batch: 5599 - Loss: 0.007354412376880646\n","Epoch: 3, Batch: 5799 - Loss: 0.010898678459227085\n","Epoch: 3, Batch: 5999 - Loss: 0.009072395212948323\n","Epoch: 3, Batch: 6199 - Loss: 0.011679702714085579\n","Epoch: 3, Batch: 6399 - Loss: 0.010169054374098778\n","Epoch: 3, Batch: 6599 - Loss: 0.009690705128014088\n","Epoch: 3, Batch: 6799 - Loss: 0.008179213479161262\n","Epoch: 3, Batch: 6999 - Loss: 0.008310423851013184\n","Epoch: 3, Batch: 7199 - Loss: 0.011230565547943116\n","Epoch: 3, Batch: 7399 - Loss: 0.0077080514505505566\n","Epoch: 4, Batch: 199 - Loss: 0.005573988035321236\n","Epoch: 4, Batch: 399 - Loss: 0.007098507232964039\n","Epoch: 4, Batch: 599 - Loss: 0.006291804261505603\n","Epoch: 4, Batch: 799 - Loss: 0.006649119302630424\n","Epoch: 4, Batch: 999 - Loss: 0.0069676209166646\n","Epoch: 4, Batch: 1199 - Loss: 0.007794603765010834\n","Epoch: 4, Batch: 1399 - Loss: 0.007309870928525925\n","Epoch: 4, Batch: 1599 - Loss: 0.010146338291466236\n","Epoch: 4, Batch: 1799 - Loss: 0.00792522194981575\n","Epoch: 4, Batch: 1999 - Loss: 0.007612146355211734\n","Epoch: 4, Batch: 2199 - Loss: 0.007555575959384441\n","Epoch: 4, Batch: 2399 - Loss: 0.007265651367604733\n","Epoch: 4, Batch: 2599 - Loss: 0.00839245493710041\n","Epoch: 4, Batch: 2799 - Loss: 0.007596781082451344\n","Epoch: 4, Batch: 2999 - Loss: 0.006372287787497043\n","Epoch: 4, Batch: 3199 - Loss: 0.008471468552947045\n","Epoch: 4, Batch: 3399 - Loss: 0.006023612223565579\n","Epoch: 4, Batch: 3599 - Loss: 0.007678086996078491\n","Epoch: 4, Batch: 3799 - Loss: 0.009112020336091518\n","Epoch: 4, Batch: 3999 - Loss: 0.008958300732076168\n","Epoch: 4, Batch: 4199 - Loss: 0.009707933582365514\n","Epoch: 4, Batch: 4399 - Loss: 0.008343049705028534\n","Epoch: 4, Batch: 4599 - Loss: 0.008874200783669949\n","Epoch: 4, Batch: 4799 - Loss: 0.007196916103363037\n","Epoch: 4, Batch: 4999 - Loss: 0.010168964080512523\n","Epoch: 4, Batch: 5199 - Loss: 0.0074815728738904\n","Epoch: 4, Batch: 5399 - Loss: 0.007148833200335503\n","Epoch: 4, Batch: 5599 - Loss: 0.008912857197225094\n","Epoch: 4, Batch: 5799 - Loss: 0.008202134616672992\n","Epoch: 4, Batch: 5999 - Loss: 0.00755552040040493\n","Epoch: 4, Batch: 6199 - Loss: 0.004260631278157234\n","Epoch: 4, Batch: 6399 - Loss: 0.007884157717227936\n","Epoch: 4, Batch: 6599 - Loss: 0.007468428798019886\n","Epoch: 4, Batch: 6799 - Loss: 0.00829013780504465\n","Epoch: 4, Batch: 6999 - Loss: 0.008424118660390377\n","Epoch: 4, Batch: 7199 - Loss: 0.006702519468963147\n","Epoch: 4, Batch: 7399 - Loss: 0.007758984923362732\n","Epoch: 5, Batch: 199 - Loss: 0.005684301435947418\n","Epoch: 5, Batch: 399 - Loss: 0.0075349750816822054\n","Epoch: 5, Batch: 599 - Loss: 0.006796712711453438\n","Epoch: 5, Batch: 799 - Loss: 0.005815762259066105\n","Epoch: 5, Batch: 999 - Loss: 0.006353474512696266\n","Epoch: 5, Batch: 1199 - Loss: 0.00533170960098505\n","Epoch: 5, Batch: 1399 - Loss: 0.006961178317666053\n","Epoch: 5, Batch: 1599 - Loss: 0.005949140876531601\n","Epoch: 5, Batch: 1799 - Loss: 0.0058834366053342815\n","Epoch: 5, Batch: 1999 - Loss: 0.0076362151503562925\n","Epoch: 5, Batch: 2199 - Loss: 0.005599018886685372\n","Epoch: 5, Batch: 2399 - Loss: 0.0060315879657864575\n","Epoch: 5, Batch: 2599 - Loss: 0.0053140252530574795\n","Epoch: 5, Batch: 2799 - Loss: 0.006416442200541496\n","Epoch: 5, Batch: 2999 - Loss: 0.006667650021612645\n","Epoch: 5, Batch: 3199 - Loss: 0.006386517398059368\n","Epoch: 5, Batch: 3399 - Loss: 0.00579439976811409\n","Epoch: 5, Batch: 3599 - Loss: 0.00844775278866291\n","Epoch: 5, Batch: 3799 - Loss: 0.006130628868937492\n","Epoch: 5, Batch: 3999 - Loss: 0.005429171115159988\n","Epoch: 5, Batch: 4199 - Loss: 0.008410946488380432\n","Epoch: 5, Batch: 4399 - Loss: 0.004927158333361149\n","Epoch: 5, Batch: 4599 - Loss: 0.0066736342534422875\n","Epoch: 5, Batch: 4799 - Loss: 0.007257496900856495\n","Epoch: 5, Batch: 4999 - Loss: 0.005135500729084015\n","Epoch: 5, Batch: 5199 - Loss: 0.007280005887150764\n","Epoch: 5, Batch: 5399 - Loss: 0.006084145352244377\n","Epoch: 5, Batch: 5599 - Loss: 0.0049300936684012414\n","Epoch: 5, Batch: 5799 - Loss: 0.007409676924347877\n","Epoch: 5, Batch: 5999 - Loss: 0.00553580217808485\n","Epoch: 5, Batch: 6199 - Loss: 0.005084056094288826\n","Epoch: 5, Batch: 6399 - Loss: 0.007398875363171101\n","Epoch: 5, Batch: 6599 - Loss: 0.006242079101502896\n","Epoch: 5, Batch: 6799 - Loss: 0.007245730467140675\n","Epoch: 5, Batch: 6999 - Loss: 0.005932655028998852\n","Epoch: 5, Batch: 7199 - Loss: 0.006122756861150265\n","Epoch: 5, Batch: 7399 - Loss: 0.0070184580832719804\n","Epoch: 6, Batch: 199 - Loss: 0.005647054493427277\n","Epoch: 6, Batch: 399 - Loss: 0.005560556642711162\n","Epoch: 6, Batch: 599 - Loss: 0.0045728861913084985\n","Epoch: 6, Batch: 799 - Loss: 0.0059949254989624026\n","Epoch: 6, Batch: 999 - Loss: 0.0042519810348749165\n","Epoch: 6, Batch: 1199 - Loss: 0.005939539045095444\n","Epoch: 6, Batch: 1399 - Loss: 0.004557979173958302\n","Epoch: 6, Batch: 1599 - Loss: 0.004708864286541939\n","Epoch: 6, Batch: 1799 - Loss: 0.005829534165561199\n","Epoch: 6, Batch: 1999 - Loss: 0.0062025637626647945\n","Epoch: 6, Batch: 2199 - Loss: 0.005625348024070263\n","Epoch: 6, Batch: 2399 - Loss: 0.0057399837225675585\n","Epoch: 6, Batch: 2599 - Loss: 0.0045613785535097125\n","Epoch: 6, Batch: 2799 - Loss: 0.006852084174752236\n","Epoch: 6, Batch: 2999 - Loss: 0.004814341463148594\n","Epoch: 6, Batch: 3199 - Loss: 0.006761096507310867\n","Epoch: 6, Batch: 3399 - Loss: 0.004573096841573715\n","Epoch: 6, Batch: 3599 - Loss: 0.0035124839395284653\n","Epoch: 6, Batch: 3799 - Loss: 0.007484254814684391\n","Epoch: 6, Batch: 3999 - Loss: 0.00687125326693058\n","Epoch: 6, Batch: 4199 - Loss: 0.0053610994592309\n","Epoch: 6, Batch: 4399 - Loss: 0.005048328697681427\n","Epoch: 6, Batch: 4599 - Loss: 0.006788688749074936\n","Epoch: 6, Batch: 4799 - Loss: 0.005413763150572777\n","Epoch: 6, Batch: 4999 - Loss: 0.005503108054399491\n","Epoch: 6, Batch: 5199 - Loss: 0.0050760069638490675\n","Epoch: 6, Batch: 5399 - Loss: 0.004207140997052192\n","Epoch: 6, Batch: 5599 - Loss: 0.0042915790677070615\n","Epoch: 6, Batch: 5799 - Loss: 0.004761502966284752\n","Epoch: 6, Batch: 5999 - Loss: 0.0039023219496011736\n","Epoch: 6, Batch: 6199 - Loss: 0.006836218297481537\n","Epoch: 6, Batch: 6399 - Loss: 0.005648833952844143\n","Epoch: 6, Batch: 6599 - Loss: 0.0052798080742359164\n","Epoch: 6, Batch: 6799 - Loss: 0.006050099313259125\n","Epoch: 6, Batch: 6999 - Loss: 0.006218990243971348\n","Epoch: 6, Batch: 7199 - Loss: 0.006440068379044533\n","Epoch: 6, Batch: 7399 - Loss: 0.004529369227588177\n","Epoch: 7, Batch: 199 - Loss: 0.004315778404474258\n","Epoch: 7, Batch: 399 - Loss: 0.003333025559782982\n","Epoch: 7, Batch: 599 - Loss: 0.005932246670126915\n","Epoch: 7, Batch: 799 - Loss: 0.005476890176534653\n","Epoch: 7, Batch: 999 - Loss: 0.004954804100096226\n","Epoch: 7, Batch: 1199 - Loss: 0.0035786173343658446\n","Epoch: 7, Batch: 1399 - Loss: 0.0050642512291669845\n","Epoch: 7, Batch: 1599 - Loss: 0.004387099206447601\n","Epoch: 7, Batch: 1799 - Loss: 0.006794781133532524\n","Epoch: 7, Batch: 1999 - Loss: 0.006363568544387818\n","Epoch: 7, Batch: 2199 - Loss: 0.004595684692263603\n","Epoch: 7, Batch: 2399 - Loss: 0.00481485864520073\n","Epoch: 7, Batch: 2599 - Loss: 0.003474499672651291\n","Epoch: 7, Batch: 2799 - Loss: 0.004713053032755852\n","Epoch: 7, Batch: 2999 - Loss: 0.0049457746893167495\n","Epoch: 7, Batch: 3199 - Loss: 0.0034376873522996903\n","Epoch: 7, Batch: 3399 - Loss: 0.0043279980272054675\n","Epoch: 7, Batch: 3599 - Loss: 0.006519493252038956\n","Epoch: 7, Batch: 3799 - Loss: 0.006753913342952728\n","Epoch: 7, Batch: 3999 - Loss: 0.0044480255842208865\n","Epoch: 7, Batch: 4199 - Loss: 0.004425979465246201\n","Epoch: 7, Batch: 4399 - Loss: 0.004926665998995304\n","Epoch: 7, Batch: 4599 - Loss: 0.006349711582064628\n","Epoch: 7, Batch: 4799 - Loss: 0.005910922057926655\n","Epoch: 7, Batch: 4999 - Loss: 0.0052544232457876205\n","Epoch: 7, Batch: 5199 - Loss: 0.004807710826396942\n","Epoch: 7, Batch: 5399 - Loss: 0.004234328836202622\n","Epoch: 7, Batch: 5599 - Loss: 0.004256463304162026\n","Epoch: 7, Batch: 5799 - Loss: 0.0043374521881341934\n","Epoch: 7, Batch: 5999 - Loss: 0.0037406399622559546\n","Epoch: 7, Batch: 6199 - Loss: 0.004557299390435219\n","Epoch: 7, Batch: 6399 - Loss: 0.005426167517900467\n","Epoch: 7, Batch: 6599 - Loss: 0.00509000289440155\n","Epoch: 7, Batch: 6799 - Loss: 0.005572237715125084\n","Epoch: 7, Batch: 6999 - Loss: 0.004375825315713883\n","Epoch: 7, Batch: 7199 - Loss: 0.00494965760409832\n","Epoch: 7, Batch: 7399 - Loss: 0.003845047667622566\n","Epoch: 8, Batch: 199 - Loss: 0.004566872872412205\n","Epoch: 8, Batch: 399 - Loss: 0.003211165264248848\n","Epoch: 8, Batch: 599 - Loss: 0.004715230092406273\n","Epoch: 8, Batch: 799 - Loss: 0.005407032072544098\n","Epoch: 8, Batch: 999 - Loss: 0.004516192376613617\n","Epoch: 8, Batch: 1199 - Loss: 0.002899538621306419\n","Epoch: 8, Batch: 1399 - Loss: 0.005093517854809761\n","Epoch: 8, Batch: 1599 - Loss: 0.002971123889088631\n","Epoch: 8, Batch: 1799 - Loss: 0.0037833221107721328\n","Epoch: 8, Batch: 1999 - Loss: 0.004893274538218975\n","Epoch: 8, Batch: 2199 - Loss: 0.005324489161372185\n","Epoch: 8, Batch: 2399 - Loss: 0.002126129075884819\n","Epoch: 8, Batch: 2599 - Loss: 0.004683120243251323\n","Epoch: 8, Batch: 2799 - Loss: 0.004293345987796783\n","Epoch: 8, Batch: 2999 - Loss: 0.004307032033801079\n","Epoch: 8, Batch: 3199 - Loss: 0.004664518646895885\n","Epoch: 8, Batch: 3399 - Loss: 0.0032457626461982725\n","Epoch: 8, Batch: 3599 - Loss: 0.003478833466768265\n","Epoch: 8, Batch: 3799 - Loss: 0.005434927046298981\n","Epoch: 8, Batch: 3999 - Loss: 0.0041197210103273395\n","Epoch: 8, Batch: 4199 - Loss: 0.003993890777230263\n","Epoch: 8, Batch: 4399 - Loss: 0.003965963304042816\n","Epoch: 8, Batch: 4599 - Loss: 0.004203299537301064\n","Epoch: 8, Batch: 4799 - Loss: 0.00469048760831356\n","Epoch: 8, Batch: 4999 - Loss: 0.004357488706707955\n","Epoch: 8, Batch: 5199 - Loss: 0.004719369560480118\n","Epoch: 8, Batch: 5399 - Loss: 0.004240018934011459\n","Epoch: 8, Batch: 5599 - Loss: 0.006781891986727714\n","Epoch: 8, Batch: 5799 - Loss: 0.005480114467442036\n","Epoch: 8, Batch: 5999 - Loss: 0.0029851377606391906\n","Epoch: 8, Batch: 6199 - Loss: 0.006000691875815391\n","Epoch: 8, Batch: 6399 - Loss: 0.006197422936558723\n","Epoch: 8, Batch: 6599 - Loss: 0.005049231052398681\n","Epoch: 8, Batch: 6799 - Loss: 0.0037978080213069915\n","Epoch: 8, Batch: 6999 - Loss: 0.005930687874555588\n","Epoch: 8, Batch: 7199 - Loss: 0.0046782766953110694\n","Epoch: 8, Batch: 7399 - Loss: 0.004921527542173862\n","Epoch: 9, Batch: 199 - Loss: 0.0026671690493822097\n","Epoch: 9, Batch: 399 - Loss: 0.0035235884338617324\n","Epoch: 9, Batch: 599 - Loss: 0.0036998547837138176\n","Epoch: 9, Batch: 799 - Loss: 0.0021766902804374695\n","Epoch: 9, Batch: 999 - Loss: 0.0030535016357898714\n","Epoch: 9, Batch: 1199 - Loss: 0.0023374424129724504\n","Epoch: 9, Batch: 1399 - Loss: 0.0028216137140989303\n","Epoch: 9, Batch: 1599 - Loss: 0.0026189676374197006\n","Epoch: 9, Batch: 1799 - Loss: 0.003837925150990486\n","Epoch: 9, Batch: 1999 - Loss: 0.0029534623622894285\n","Epoch: 9, Batch: 2199 - Loss: 0.002943678192794323\n","Epoch: 9, Batch: 2399 - Loss: 0.005950994223356247\n","Epoch: 9, Batch: 2599 - Loss: 0.005027890808880329\n","Epoch: 9, Batch: 2799 - Loss: 0.00314725099503994\n","Epoch: 9, Batch: 2999 - Loss: 0.0034437144696712494\n","Epoch: 9, Batch: 3199 - Loss: 0.004088822871446609\n","Epoch: 9, Batch: 3399 - Loss: 0.0027204805612564085\n","Epoch: 9, Batch: 3599 - Loss: 0.004057330638170242\n","Epoch: 9, Batch: 3799 - Loss: 0.004992861896753311\n","Epoch: 9, Batch: 3999 - Loss: 0.0035343417674303054\n","Epoch: 9, Batch: 4199 - Loss: 0.0033009398728609087\n","Epoch: 9, Batch: 4399 - Loss: 0.003912357494235039\n","Epoch: 9, Batch: 4599 - Loss: 0.004596064589917659\n","Epoch: 9, Batch: 4799 - Loss: 0.0049146337658166886\n","Epoch: 9, Batch: 4999 - Loss: 0.0034752761647105215\n","Epoch: 9, Batch: 5199 - Loss: 0.004206741616129875\n","Epoch: 9, Batch: 5399 - Loss: 0.004508067414164543\n","Epoch: 9, Batch: 5599 - Loss: 0.003338009685277939\n","Epoch: 9, Batch: 5799 - Loss: 0.0028238039314746857\n","Epoch: 9, Batch: 5999 - Loss: 0.003643468879163265\n","Epoch: 9, Batch: 6199 - Loss: 0.005481889992952346\n","Epoch: 9, Batch: 6399 - Loss: 0.004331762596964836\n","Epoch: 9, Batch: 6599 - Loss: 0.003971032693982125\n","Epoch: 9, Batch: 6799 - Loss: 0.0052106062322854994\n","Epoch: 9, Batch: 6999 - Loss: 0.00442555620521307\n","Epoch: 9, Batch: 7199 - Loss: 0.0043916359767317776\n","Epoch: 9, Batch: 7399 - Loss: 0.0044631876796484\n","Finished Training\n"],"name":"stdout"}]},{"metadata":{"id":"bkCCsZwxCSsq","colab_type":"code","outputId":"d9cd7de4-620a-4981-ed48-a0bebd623620","executionInfo":{"status":"ok","timestamp":1551541636224,"user_tz":-360,"elapsed":5772,"user":{"displayName":"Birzhan Moldagaliyev","photoUrl":"https://lh6.googleusercontent.com/-bBMNk_lfHxc/AAAAAAAAAAI/AAAAAAAAALw/Z9rp6VxhrVw/s64/photo.jpg","userId":"07030169108509602369"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# testing model_gpu\n","total = 0\n","correct = 0\n","with torch.no_grad():\n","  for data in test_loader:\n","    inputs, labels = data\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","    outputs = model_gpu(inputs)\n","    _, predicted = torch.max(outputs.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","print('Accuracy of the network on 10,000 images is {}'.format(correct/total))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy of the network on 10,000 images is 0.9826\n"],"name":"stdout"}]}]}