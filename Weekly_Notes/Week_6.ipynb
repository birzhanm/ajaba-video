{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9lF1MCl2wDIv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Building an Image Captioning System (Part 1 of 2)\n",
        "Before building a video captioning system, we could try to build an image captioning system. \n",
        "An image captioning system takes image as an input, and it should produce the image's caption. Most of modern image captioning system based on Deep Learning employ two-stage model:\n",
        "1. Capture important features from the image. For this one could use CNN.\n",
        "1. Feed this feature vector into RNN to generate a caption.\n",
        "\n",
        "In this notebook we are going to delve into details of how it can be done.\n",
        "\n",
        "We study the image captioning system built by Yunjey Choi, and <a href=\"https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning\">here</a> is the link to the corresponding GitHub repo with image captioning project. \n",
        "\n",
        "This notebook can be considered as a joint work of Alibek Orynbassar and Birzhan Moldagaliyev"
      ]
    },
    {
      "metadata": {
        "id": "JclS02R1wCg5",
        "colab_type": "code",
        "outputId": "4b1ee3d6-602d-4e6f-dc69-2c370634ea9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2204
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q torch==1.0.0 torchvision\n",
        "\n",
        "# let us try to understand the CNN part first\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, embed_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    resnet18 = models.resnet18(pretrained=True)\n",
        "    modules = list(resnet18.children())[:-1]\n",
        "    self.resnet = nn.Sequential(*modules)\n",
        "    self.linear = nn.Linear(resnet18.fc.in_features, embed_size)\n",
        "    self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "    \n",
        "  def forward(self, images):\n",
        "    with torch.no_grad():\n",
        "      features = self.resnet(images)\n",
        "    features = features.view(features.size(0), -1)\n",
        "    features = self.linear(features)\n",
        "    features = self.bn(features)\n",
        "    return features\n",
        "\n",
        "# let us perform a sanity check\n",
        "embed_size = 300\n",
        "images = torch.randn(2, 3, 224, 224)\n",
        "\n",
        "encoder = Encoder(embed_size)\n",
        "print(encoder(images))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 591.8MB 26kB/s \n",
            "\u001b[31mfastai 1.0.51 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.torch/models/resnet18-5c106cde.pth\n",
            "100%|██████████| 46827520/46827520 [00:01<00:00, 24638957.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.6998e-01, -9.8909e-01, -3.7659e-01,  5.2393e-01, -6.5969e-01,\n",
            "         -2.4887e-01, -9.9046e-01, -1.7060e-01,  5.1260e-01,  7.3862e-01,\n",
            "         -1.1618e-01,  8.6441e-01,  5.3538e-01,  8.0630e-02,  8.9902e-01,\n",
            "         -1.3806e-01, -5.5699e-02,  2.8007e-01,  2.2073e-01,  4.9998e-01,\n",
            "          7.6076e-01,  4.4532e-01,  5.4514e-01,  8.3451e-01,  1.7190e-01,\n",
            "         -9.5332e-01, -4.9066e-01,  8.8761e-01, -9.5369e-01, -3.3473e-01,\n",
            "          3.8383e-01, -3.8949e-01,  8.5460e-01,  8.9659e-01,  4.9491e-01,\n",
            "         -5.1683e-01, -4.9997e-01, -3.1168e-01, -5.3843e-01, -9.2439e-02,\n",
            "          2.1483e-01, -8.5620e-01,  8.2038e-01, -4.9159e-01, -3.8859e-01,\n",
            "         -2.0815e-01,  3.4762e-01, -4.2014e-01, -7.3013e-01, -1.6654e-01,\n",
            "         -9.1146e-01, -2.8602e-01,  7.9210e-01, -9.7904e-01, -6.6130e-01,\n",
            "          1.5624e-01, -5.5582e-01, -9.1548e-01, -4.0081e-01,  1.5074e-01,\n",
            "          2.8860e-01,  2.5784e-01, -3.5420e-01, -3.9437e-01,  1.2254e-01,\n",
            "         -4.3627e-01,  7.5056e-01,  4.8882e-01, -1.6191e-01, -8.5203e-01,\n",
            "          1.7758e-01, -1.2985e-01, -9.2265e-02, -8.6985e-01,  6.4910e-01,\n",
            "         -1.5799e-01,  3.3340e-01, -2.6817e-01, -6.7939e-01,  9.7223e-01,\n",
            "         -3.7514e-01,  2.8949e-01, -1.5569e-01,  1.5695e-02, -2.9081e-02,\n",
            "          6.5193e-01, -8.1245e-01,  1.3241e-01, -1.0589e-01, -8.1929e-01,\n",
            "         -6.1252e-01,  3.9514e-01, -7.9449e-01,  1.7707e-01,  9.2111e-01,\n",
            "         -8.5463e-01,  3.9402e-02,  8.1325e-02, -6.8328e-01,  2.4403e-01,\n",
            "          3.7037e-01,  8.6686e-01,  3.7064e-01, -5.9154e-01,  8.5371e-02,\n",
            "          4.3777e-01,  3.8200e-01,  4.5523e-01,  8.6938e-02,  5.0547e-01,\n",
            "          4.8463e-01,  2.1464e-03,  3.3290e-01, -7.3698e-02,  4.3461e-01,\n",
            "         -3.1229e-01, -1.7967e-01, -7.4127e-01,  6.3973e-01,  9.5805e-01,\n",
            "          7.5955e-01, -3.1442e-01,  8.3971e-01,  1.4587e-01, -7.6126e-01,\n",
            "          1.2499e-01,  4.2899e-01,  4.8990e-01, -7.9792e-01,  5.9403e-02,\n",
            "          1.8489e-02,  8.1152e-01, -7.8338e-01, -2.6862e-01, -9.2069e-01,\n",
            "          5.8886e-01, -7.6488e-01,  8.5056e-01, -5.5079e-01, -6.5870e-01,\n",
            "         -2.3922e-01, -6.1394e-01,  7.0277e-01,  2.3061e-01, -5.1262e-01,\n",
            "          7.4394e-01, -7.9224e-01, -9.7076e-01, -8.8747e-01, -4.1979e-01,\n",
            "          4.2162e-01, -3.7522e-01, -9.0709e-02, -3.8640e-01, -9.3352e-01,\n",
            "         -9.6720e-01,  5.7055e-01, -7.0035e-01, -2.0946e-01,  7.0062e-01,\n",
            "          6.1510e-01,  2.6963e-01,  5.5038e-02,  6.6595e-01,  4.7631e-01,\n",
            "         -6.9634e-01,  4.6679e-01,  1.7585e-01,  5.4703e-01, -4.7064e-01,\n",
            "          7.5251e-01, -8.6434e-01,  8.6195e-01, -5.8644e-01,  6.1891e-01,\n",
            "         -1.1055e-01, -2.5465e-01,  1.9190e-01,  3.6900e-01, -1.8723e-02,\n",
            "         -1.6745e-01, -2.8124e-04,  3.9653e-01, -9.7054e-01,  7.0537e-01,\n",
            "         -8.1855e-01,  9.5526e-01,  1.4222e-01,  6.3679e-01,  4.8075e-01,\n",
            "         -2.2075e-01, -4.1342e-01,  9.9983e-01, -5.2396e-01, -9.9410e-01,\n",
            "          9.8580e-01,  5.1087e-01, -3.3044e-01, -8.2944e-02,  7.1721e-02,\n",
            "         -4.3576e-01,  4.6883e-01,  3.0022e-01,  6.1778e-01, -7.5824e-01,\n",
            "         -9.1053e-02,  8.1698e-01,  9.9120e-01, -2.5123e-01,  5.8947e-01,\n",
            "          1.9662e-01, -9.4076e-01, -9.5734e-01,  8.2244e-01,  7.7444e-01,\n",
            "          7.1617e-02,  8.3230e-01, -7.6857e-01, -8.7245e-01,  2.8962e-01,\n",
            "          7.9402e-01, -7.8167e-01,  8.2803e-01,  7.1249e-02,  9.2891e-01,\n",
            "          5.1352e-01,  8.9645e-01, -5.4272e-02,  8.6780e-01, -4.8176e-01,\n",
            "          1.4658e-01,  3.0207e-01,  1.8845e-01,  5.8200e-01, -7.8530e-01,\n",
            "          4.9625e-01,  1.2200e-01, -2.8604e-01, -6.6841e-01,  3.2448e-01,\n",
            "          5.8625e-01, -5.0774e-01,  2.3426e-01,  6.2450e-01,  9.5334e-01,\n",
            "          7.1302e-01,  3.8797e-01, -9.0445e-01,  7.2806e-01,  7.9449e-01,\n",
            "         -9.3278e-01,  6.0902e-01,  5.0904e-01, -8.8160e-01, -8.9812e-01,\n",
            "         -6.2587e-01,  2.2040e-01, -1.9079e-01, -5.8418e-01, -2.0190e-01,\n",
            "          1.8624e-01,  7.4239e-02,  7.5802e-01, -9.3060e-01, -9.0773e-01,\n",
            "         -2.1823e-01,  5.8013e-02, -2.2291e-01, -9.6125e-01, -2.6476e-01,\n",
            "          1.1562e-01, -7.5173e-01,  4.3581e-01, -6.8280e-01, -8.5642e-01,\n",
            "          2.0361e-01, -8.9911e-01,  9.6363e-01, -3.5860e-01, -4.4298e-01,\n",
            "          5.9779e-01,  7.4271e-01, -9.5720e-01,  6.9336e-01,  9.3816e-01,\n",
            "          9.2103e-01,  2.4011e-01,  9.7042e-01,  2.6671e-01, -6.1188e-01,\n",
            "         -2.2043e-01, -3.0175e-01,  5.5568e-01,  4.8212e-01,  1.5108e-01,\n",
            "         -4.1624e-01, -5.6925e-01, -8.6912e-01, -9.6894e-01,  2.5760e-01],\n",
            "        [-2.6998e-01,  9.8909e-01,  3.7659e-01, -5.2393e-01,  6.5969e-01,\n",
            "          2.4887e-01,  9.9046e-01,  1.7060e-01, -5.1260e-01, -7.3862e-01,\n",
            "          1.1618e-01, -8.6441e-01, -5.3538e-01, -8.0627e-02, -8.9902e-01,\n",
            "          1.3806e-01,  5.5699e-02, -2.8007e-01, -2.2073e-01, -4.9998e-01,\n",
            "         -7.6076e-01, -4.4532e-01, -5.4514e-01, -8.3451e-01, -1.7190e-01,\n",
            "          9.5332e-01,  4.9066e-01, -8.8761e-01,  9.5369e-01,  3.3473e-01,\n",
            "         -3.8383e-01,  3.8949e-01, -8.5460e-01, -8.9660e-01, -4.9491e-01,\n",
            "          5.1683e-01,  4.9997e-01,  3.1168e-01,  5.3843e-01,  9.2439e-02,\n",
            "         -2.1483e-01,  8.5620e-01, -8.2038e-01,  4.9159e-01,  3.8859e-01,\n",
            "          2.0815e-01, -3.4762e-01,  4.2014e-01,  7.3013e-01,  1.6654e-01,\n",
            "          9.1146e-01,  2.8602e-01, -7.9210e-01,  9.7904e-01,  6.6130e-01,\n",
            "         -1.5624e-01,  5.5582e-01,  9.1548e-01,  4.0081e-01, -1.5074e-01,\n",
            "         -2.8860e-01, -2.5784e-01,  3.5420e-01,  3.9437e-01, -1.2254e-01,\n",
            "          4.3627e-01, -7.5056e-01, -4.8882e-01,  1.6191e-01,  8.5203e-01,\n",
            "         -1.7758e-01,  1.2985e-01,  9.2265e-02,  8.6985e-01, -6.4910e-01,\n",
            "          1.5799e-01, -3.3340e-01,  2.6817e-01,  6.7939e-01, -9.7223e-01,\n",
            "          3.7514e-01, -2.8949e-01,  1.5569e-01, -1.5695e-02,  2.9081e-02,\n",
            "         -6.5193e-01,  8.1245e-01, -1.3241e-01,  1.0589e-01,  8.1929e-01,\n",
            "          6.1252e-01, -3.9514e-01,  7.9449e-01, -1.7707e-01, -9.2111e-01,\n",
            "          8.5463e-01, -3.9402e-02, -8.1325e-02,  6.8328e-01, -2.4403e-01,\n",
            "         -3.7037e-01, -8.6686e-01, -3.7064e-01,  5.9154e-01, -8.5371e-02,\n",
            "         -4.3777e-01, -3.8200e-01, -4.5523e-01, -8.6938e-02, -5.0547e-01,\n",
            "         -4.8463e-01, -2.1464e-03, -3.3290e-01,  7.3698e-02, -4.3461e-01,\n",
            "          3.1229e-01,  1.7967e-01,  7.4127e-01, -6.3973e-01, -9.5805e-01,\n",
            "         -7.5955e-01,  3.1442e-01, -8.3971e-01, -1.4587e-01,  7.6126e-01,\n",
            "         -1.2499e-01, -4.2899e-01, -4.8990e-01,  7.9792e-01, -5.9403e-02,\n",
            "         -1.8489e-02, -8.1152e-01,  7.8338e-01,  2.6862e-01,  9.2069e-01,\n",
            "         -5.8886e-01,  7.6488e-01, -8.5056e-01,  5.5079e-01,  6.5870e-01,\n",
            "          2.3922e-01,  6.1394e-01, -7.0277e-01, -2.3061e-01,  5.1262e-01,\n",
            "         -7.4394e-01,  7.9224e-01,  9.7076e-01,  8.8747e-01,  4.1979e-01,\n",
            "         -4.2162e-01,  3.7522e-01,  9.0709e-02,  3.8640e-01,  9.3352e-01,\n",
            "          9.6720e-01, -5.7055e-01,  7.0035e-01,  2.0946e-01, -7.0062e-01,\n",
            "         -6.1510e-01, -2.6963e-01, -5.5038e-02, -6.6595e-01, -4.7631e-01,\n",
            "          6.9634e-01, -4.6679e-01, -1.7585e-01, -5.4703e-01,  4.7064e-01,\n",
            "         -7.5251e-01,  8.6434e-01, -8.6195e-01,  5.8644e-01, -6.1891e-01,\n",
            "          1.1055e-01,  2.5465e-01, -1.9190e-01, -3.6900e-01,  1.8723e-02,\n",
            "          1.6745e-01,  2.8124e-04, -3.9653e-01,  9.7054e-01, -7.0537e-01,\n",
            "          8.1855e-01, -9.5526e-01, -1.4222e-01, -6.3679e-01, -4.8075e-01,\n",
            "          2.2075e-01,  4.1342e-01, -9.9983e-01,  5.2396e-01,  9.9410e-01,\n",
            "         -9.8580e-01, -5.1087e-01,  3.3044e-01,  8.2944e-02, -7.1721e-02,\n",
            "          4.3576e-01, -4.6883e-01, -3.0022e-01, -6.1779e-01,  7.5824e-01,\n",
            "          9.1053e-02, -8.1698e-01, -9.9120e-01,  2.5123e-01, -5.8947e-01,\n",
            "         -1.9662e-01,  9.4076e-01,  9.5734e-01, -8.2244e-01, -7.7444e-01,\n",
            "         -7.1617e-02, -8.3230e-01,  7.6857e-01,  8.7245e-01, -2.8962e-01,\n",
            "         -7.9402e-01,  7.8167e-01, -8.2803e-01, -7.1249e-02, -9.2891e-01,\n",
            "         -5.1352e-01, -8.9645e-01,  5.4272e-02, -8.6780e-01,  4.8176e-01,\n",
            "         -1.4658e-01, -3.0207e-01, -1.8845e-01, -5.8200e-01,  7.8530e-01,\n",
            "         -4.9625e-01, -1.2200e-01,  2.8604e-01,  6.6841e-01, -3.2448e-01,\n",
            "         -5.8625e-01,  5.0774e-01, -2.3426e-01, -6.2450e-01, -9.5334e-01,\n",
            "         -7.1302e-01, -3.8797e-01,  9.0445e-01, -7.2806e-01, -7.9449e-01,\n",
            "          9.3278e-01, -6.0902e-01, -5.0904e-01,  8.8160e-01,  8.9812e-01,\n",
            "          6.2587e-01, -2.2040e-01,  1.9079e-01,  5.8418e-01,  2.0190e-01,\n",
            "         -1.8624e-01, -7.4239e-02, -7.5802e-01,  9.3060e-01,  9.0773e-01,\n",
            "          2.1823e-01, -5.8013e-02,  2.2291e-01,  9.6125e-01,  2.6476e-01,\n",
            "         -1.1562e-01,  7.5173e-01, -4.3581e-01,  6.8280e-01,  8.5642e-01,\n",
            "         -2.0361e-01,  8.9911e-01, -9.6363e-01,  3.5860e-01,  4.4298e-01,\n",
            "         -5.9779e-01, -7.4271e-01,  9.5720e-01, -6.9336e-01, -9.3816e-01,\n",
            "         -9.2103e-01, -2.4011e-01, -9.7042e-01, -2.6671e-01,  6.1188e-01,\n",
            "          2.2043e-01,  3.0175e-01, -5.5568e-01, -4.8212e-01, -1.5108e-01,\n",
            "          4.1624e-01,  5.6925e-01,  8.6912e-01,  9.6894e-01, -2.5760e-01]],\n",
            "       grad_fn=<NativeBatchNormBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h3lkVF4b6Gm_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let us now attempt to understand the decoder\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_sequence_length=20):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "    self.max_seq_length = max_seq_length\n",
        "    \n",
        "  def forward(self, features, captions, lengths):\n",
        "    embeddings = self.embed(captions)\n",
        "    embeddings = torch.cat((features.unsqueeze(1),embeddings), 1)\n",
        "    packed = pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
        "    hiddens, _ = self.lstm(packed)\n",
        "    outputs = self.linear(hiddens[0])\n",
        "    \n",
        "  def sample(self, features, states=None):\n",
        "    sample_ids = []\n",
        "    inputs = features.unsqueeze(1)\n",
        "    for i in range(self.max_seq_length):\n",
        "      hiddens, states = self.lstm(inputs, states)\n",
        "      outputs = self.linear(hiddens.squeeze(1))\n",
        "      _, predicted = outputs.max(1)\n",
        "      sample_ids.append(predicted)\n",
        "      inputs = self.embed(predicted)\n",
        "      inputs.unsqueeze(1)\n",
        "    sample_ids = torch.stack(sample_ids, 1)\n",
        "    return sample_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eu8Fcgx-xZaH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Init Part\n"
      ]
    },
    {
      "metadata": {
        "id": "EYfdaq5Axg2w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## embed\n",
        "Here (embed) embeds indices of words into a lower dimensional vector. For example, suppose we have two kinds of words: 'True' and 'False'. We also need to have a symbol for empty word, which we are going to denote as < PAD >. So we have three kind of words in total: 'True', 'False' and '< PAD >'. We then going to index these words from 0 to 2. The we could use (embed) to embed these indices into two-dimensional vectors. "
      ]
    },
    {
      "metadata": {
        "id": "B_4eykIcxvjN",
        "colab_type": "code",
        "outputId": "4464c8d8-861e-49dd-d94b-2116f43462eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# batch_size = 3\n",
        "raw_stream = [['True', 'True', 'True', 'False', 'False', 'True'], ['True', 'False', 'True'], ['True', 'True']]\n",
        "stream_dic = {'<PAD>':0, 'True':1, 'False':2}\n",
        "indexed_stream = [[stream_dic[label] for label in seq] for seq in raw_stream]\n",
        "lengths = [len(seq) for seq in indexed_stream]\n",
        "pad_token = stream_dic['<PAD>']\n",
        "longest_seq = max(lengths)\n",
        "batch_size = len(indexed_stream)\n",
        "padded_stream = np.ones((batch_size, longest_seq))*pad_token\n",
        "for i, seq_len in enumerate(lengths):\n",
        "  padded_stream[i,:seq_len] = indexed_stream[i]\n",
        "padded_stream = torch.tensor(padded_stream, dtype=torch.long)\n",
        "print(padded_stream)\n",
        "\n",
        "# we can now embed the padded stream into lower dimensional representation\n",
        "initial_dim = 3\n",
        "lower_dim = 2\n",
        "embed = nn.Embedding(initial_dim, lower_dim)\n",
        "embedded_stream = embed(padded_stream)\n",
        "print(embedded_stream)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 1, 1, 2, 2, 1],\n",
            "        [1, 2, 1, 0, 0, 0],\n",
            "        [1, 1, 0, 0, 0, 0]])\n",
            "tensor([[[ 0.6346,  0.3061],\n",
            "         [ 0.6346,  0.3061],\n",
            "         [ 0.6346,  0.3061],\n",
            "         [ 0.7149, -0.0028],\n",
            "         [ 0.7149, -0.0028],\n",
            "         [ 0.6346,  0.3061]],\n",
            "\n",
            "        [[ 0.6346,  0.3061],\n",
            "         [ 0.7149, -0.0028],\n",
            "         [ 0.6346,  0.3061],\n",
            "         [-0.6702,  0.8191],\n",
            "         [-0.6702,  0.8191],\n",
            "         [-0.6702,  0.8191]],\n",
            "\n",
            "        [[ 0.6346,  0.3061],\n",
            "         [ 0.6346,  0.3061],\n",
            "         [-0.6702,  0.8191],\n",
            "         [-0.6702,  0.8191],\n",
            "         [-0.6702,  0.8191],\n",
            "         [-0.6702,  0.8191]]], grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5j6jA32B29GH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#lstm\n",
        "lstm represents a kind of recurrent neural network cell. Let's play around with a simple lstm cell. Throughout this notebook cell we assume that batch comes as a first dimension, i.e. batch_first = True"
      ]
    },
    {
      "metadata": {
        "id": "W0RP24GTl6A-",
        "colab_type": "code",
        "outputId": "7858879b-938b-43b2-9e9d-d4e45a2ff587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        }
      },
      "cell_type": "code",
      "source": [
        "# let's try to understand how does an LSTM cell work\n",
        "input_size = 3\n",
        "seq_len = 7\n",
        "hidden_size = 2\n",
        "batch_size = 3\n",
        "num_layers = 1\n",
        "num_directions = 1\n",
        "\n",
        "lstm_cell = nn.LSTM(input_size=input_size, hidden_size=hidden_size, \n",
        "                    num_layers=num_layers, batch_first=True)\n",
        "h0 = torch.randn(num_layers*num_directions, batch_size, hidden_size)\n",
        "c0 = torch.randn(num_layers*num_directions, batch_size, hidden_size)\n",
        "sample = torch.randn(batch_size, seq_len, input_size)\n",
        "out1, (hn,cn) = lstm_cell(sample, (h0,c0))\n",
        "print('out1: {}'.format(out1))\n",
        "# if one omits (h0,c0) they are set to 0 tensors\n",
        "out2, (hn,cn) = lstm_cell(sample)\n",
        "print('out2: {}'.format(out2))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "out1: tensor([[[ 0.3128, -0.1324],\n",
            "         [ 0.2009, -0.2255],\n",
            "         [-0.0066, -0.3524],\n",
            "         [ 0.1208, -0.2915],\n",
            "         [ 0.1950, -0.1549],\n",
            "         [-0.1604, -0.5076],\n",
            "         [-0.3904, -0.4340]],\n",
            "\n",
            "        [[-0.2046, -0.4704],\n",
            "         [-0.2986, -0.2940],\n",
            "         [ 0.2040, -0.2584],\n",
            "         [ 0.1660, -0.0242],\n",
            "         [ 0.1723,  0.2223],\n",
            "         [ 0.1695,  0.3431],\n",
            "         [-0.2145, -0.0970]],\n",
            "\n",
            "        [[ 0.0815, -0.0457],\n",
            "         [ 0.2478, -0.1699],\n",
            "         [-0.1294, -0.2739],\n",
            "         [ 0.2193, -0.1089],\n",
            "         [ 0.0750, -0.3292],\n",
            "         [ 0.0517, -0.3054],\n",
            "         [-0.1402, -0.3411]]], grad_fn=<TransposeBackward0>)\n",
            "out2: tensor([[[-0.0616, -0.2216],\n",
            "         [ 0.1166, -0.4554],\n",
            "         [-0.0496, -0.4591],\n",
            "         [ 0.1112, -0.3907],\n",
            "         [ 0.1878, -0.1846],\n",
            "         [-0.1666, -0.5360],\n",
            "         [-0.3946, -0.4410]],\n",
            "\n",
            "        [[-0.3823, -0.2191],\n",
            "         [-0.3518, -0.2678],\n",
            "         [ 0.1840, -0.2433],\n",
            "         [ 0.1615, -0.0217],\n",
            "         [ 0.1712,  0.2233],\n",
            "         [ 0.1692,  0.3434],\n",
            "         [-0.2149, -0.0969]],\n",
            "\n",
            "        [[ 0.1331, -0.0956],\n",
            "         [ 0.2585, -0.1948],\n",
            "         [-0.1294, -0.2790],\n",
            "         [ 0.2194, -0.1154],\n",
            "         [ 0.0741, -0.3310],\n",
            "         [ 0.0513, -0.3061],\n",
            "         [-0.1404, -0.3414]]], grad_fn=<TransposeBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sdadCnb481Nq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##linear\n",
        "A usual linear transformation layer"
      ]
    },
    {
      "metadata": {
        "id": "VStcJwmn8_8W",
        "colab_type": "code",
        "outputId": "e6eeb0fe-6d8b-48ca-e618-2b5c8c0ec256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 3\n",
        "input_size = 3\n",
        "output_size = 4\n",
        "\n",
        "linear = nn.Linear(input_size, output_size)\n",
        "sample_input = torch.randn(batch_size, input_size)\n",
        "output = linear(sample_input)\n",
        "print(output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.3086, -0.5280,  0.0065,  1.4702],\n",
            "        [ 0.1297, -0.4131, -0.1351,  0.1972],\n",
            "        [ 0.2923, -0.6071, -0.6792,  0.4919]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9L318ANg-cyX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##max_seq_length\n",
        "max_seq_length sets the upper bound for the number of words in the generated caption"
      ]
    },
    {
      "metadata": {
        "id": "gQW-oFcb_tK-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Forward Part\n",
        "In order to understand the forward pass, we could come up with some synthetic data to feed and see how does the forward pass work. Let us try to generate a sequence of nucleic acids based on some given feature. There are 4 types of nucleic acids denoted as: 'G', 'C', 'A' and 'T'. We also have a padding symbol '< PAD >'. So, in total we have 5 symbols. We could use the following indexing to turn symbols into integers: {< PAD >: 0, G: 1, C: 2, A: 3, T: 4}.  \n"
      ]
    },
    {
      "metadata": {
        "id": "8hycBueMADIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set up data\n",
        "vocab_size = 5\n",
        "batch_size = 2\n",
        "feature_size = 4\n",
        "embed_size = 4\n",
        "hidden_size = 3\n",
        "num_layers = 1\n",
        "max_seq_length = 6\n",
        "\n",
        "# Features. Since the batch size is 2, we need to generate 2 feature vectors of size 4\n",
        "features = torch.randn(batch_size, feature_size) \n",
        "\n",
        "# Captions. Suppose that gene sequences corresponding to these features are as follows\n",
        "# feature 1: [G, C, T, T, A, C] or [1, 2, 4, 4, 3, 2] using the indexing\n",
        "# feature 2: [T, G, G, A] or [4, 1, 1, 3] using the indexing]\n",
        "caption_1 = [1, 2, 4, 4, 3, 2]\n",
        "caption_2 = [4, 1, 1, 3]\n",
        "# a variable lengths stores lengths of captions\n",
        "lengths = [6, 4]\n",
        "# we need to pad caption_2, to make lengths of captions to be equal\n",
        "caption_2 = [4, 1, 1, 3, 0, 0]\n",
        "captions = torch.tensor([caption_1, caption_2], dtype=torch.long)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edpUKvrYgHA9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set up infrastructure\n",
        "embed = nn.Embedding(vocab_size, embed_size)\n",
        "lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "linear = nn.Linear(hidden_size, vocab_size)\n",
        "max_seq_length = max_seq_length\n",
        "\n",
        "# set up rnn utilities\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4pntTF7wgeCW",
        "colab_type": "code",
        "outputId": "21881742-c26f-4a5f-915b-2bdd7b5c11ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        }
      },
      "cell_type": "code",
      "source": [
        "# run forward path with printing dimensions of tensors along the way\n",
        "embeddings = embed(captions)\n",
        "print(\"embeddings' dimensions: {}\".format(embeddings.size()))\n",
        "\n",
        "# we then prepend feature vectors as the first elements of corresponding captions\n",
        "featured_embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "print(\"featured_embeddings' dimensions: {}\".format(featured_embeddings.size()))\n",
        "\n",
        "# we then pack featured_embeddings for convenience and efficiency\n",
        "packed = pack_padded_sequence(featured_embeddings, lengths, batch_first=True)\n",
        "print(\"Packed sequence output:\\n{}\".format(packed))\n",
        "\n",
        "# we then run packed sequence through lstm getting a packed output at the end\n",
        "packed_hiddens, _ = lstm(packed)\n",
        "print(\"packed_output {}\".format(packed_hiddens))\n",
        "\n",
        "# in princile, we could carry on using the packed format provided that we remember it.\n",
        "packed_final = linear(packed_hiddens[0])\n",
        "print(\"dimensions of packed_final: {}\".format(packed_final.size()))\n",
        "\n",
        "# the alternative is to reverse packing, and only then apply the linear transformation. This way seems \n",
        "# to be more natural, but less efficient, because we will perform some unnecessary computations.\n",
        "unpacked_hiddens = pad_packed_sequence(packed_hiddens, batch_first=True)[0] # [0] index to get a tensor\n",
        "print(\"dimensions for unpacked_hidden: {}\".format(unpacked_hiddens.size()))\n",
        "\n",
        "# we can then apply the linear transformation\n",
        "unpacked_final = linear(unpacked_hiddens)\n",
        "print(\"dimensions for unpacked_final: {}\".format(unpacked_final.size()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embeddings' dimensions: torch.Size([2, 6, 4])\n",
            "featured_embeddings' dimensions: torch.Size([2, 7, 4])\n",
            "Packed sequence output:\n",
            "PackedSequence(data=tensor([[-0.7754,  0.6657,  0.9380,  0.4559],\n",
            "        [ 0.3809,  0.5994,  0.5325,  1.1054],\n",
            "        [-0.7672, -0.1533, -0.6961,  2.4644],\n",
            "        [ 0.8890,  1.3946, -1.1777,  0.2074],\n",
            "        [ 0.3553, -0.9340, -1.1730, -0.8144],\n",
            "        [-0.7672, -0.1533, -0.6961,  2.4644],\n",
            "        [ 0.8890,  1.3946, -1.1777,  0.2074],\n",
            "        [-0.7672, -0.1533, -0.6961,  2.4644],\n",
            "        [ 0.8890,  1.3946, -1.1777,  0.2074],\n",
            "        [ 1.6336,  1.2927,  0.6528, -0.2332]],\n",
            "       grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([2, 2, 2, 2, 1, 1]))\n",
            "packed_output PackedSequence(data=tensor([[ 8.9204e-02,  1.1213e-01, -1.0824e-01],\n",
            "        [ 1.2020e-01,  1.1709e-01, -5.8015e-02],\n",
            "        [ 1.8218e-01,  3.7615e-01, -1.5257e-01],\n",
            "        [ 2.4978e-01,  1.3320e-01, -2.1004e-01],\n",
            "        [ 1.9492e-01,  1.4153e-01,  1.7107e-01],\n",
            "        [ 2.8571e-01,  3.5327e-01, -1.3535e-01],\n",
            "        [ 2.9747e-01,  1.6207e-01, -6.7950e-02],\n",
            "        [ 2.4292e-01,  4.1740e-01, -9.7616e-02],\n",
            "        [ 3.0557e-01,  1.3724e-01, -1.7230e-01],\n",
            "        [ 2.3872e-01,  2.7279e-05, -1.1859e-04]], grad_fn=<CatBackward>), batch_sizes=tensor([2, 2, 2, 2, 1, 1]))\n",
            "dimensions of packed_final: torch.Size([10, 5])\n",
            "dimensions for unpacked_hidden: torch.Size([2, 6, 3])\n",
            "dimensions for unpacked_final: torch.Size([2, 6, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}